# MTG AI Training Pipeline Documentation - Task 3.4

## Overview

This document describes the comprehensive training pipeline for Magic: The Gathering AI, implementing Task 3.4: Loss Function and Training Setup. The pipeline provides outcome-weighted loss functions, multi-task learning, curriculum learning, and complete training infrastructure for optimizing the MTG AI gameplay model.

## Architecture

### Core Components

1. **Training Pipeline** (`mtg_training_pipeline.py`)
   - Outcome-weighted loss functions
   - Multi-task learning (action classification + value estimation)
   - Curriculum learning with progressive stages
   - Mixed precision training with gradient clipping
   - Early stopping and checkpointing

2. **Evaluation Metrics** (`mtg_evaluation_metrics.py`)
   - Comprehensive performance metrics
   - Calibration analysis
   - Decision type analysis
   - Performance validation

3. **Hyperparameter Optimization** (`mtg_hyperparameter_optimization.py`)
   - Grid search, random search, Bayesian optimization
   - Evolutionary algorithms
   - Cross-validation support
   - Automated parameter importance analysis

4. **Training Monitor** (`mtg_training_monitor.py`)
   - Real-time progress tracking
   - TensorBoard and W&B integration
   - Interactive dashboards
   - Performance alerts

5. **Model Versioning** (`mtg_model_versioning.py`)
   - Comprehensive checkpointing system
   - Model metadata tracking
   - Deployment validation
   - Export capabilities

## Installation and Setup

### Dependencies

```bash
# Core dependencies
pip install torch torchvision torchaudio
pip install numpy pandas scikit-learn
pip install matplotlib seaborn plotly

# Optional monitoring dependencies
pip install tensorboard
pip install wandb

# Optional hyperparameter optimization
pip install optuna
pip install scikit-optimize

# Development dependencies
pip install pytest black flake8
```

### Data Requirements

The training pipeline expects data in the format generated by Task 2.4:
- JSON file with training samples
- Each sample contains:
  - `tensor_data`: 282-dimension state tensor
  - `action_label`: 16-dimension action one-hot vector
  - `outcome_weight`: Decision importance weight
  - `game_outcome`: Boolean game result
  - `decision_type`: Strategic context
  - `strategic_context`: Additional game state information

## Usage Guide

### Basic Training

```python
from mtg_training_pipeline import MTGTrainer, TrainingConfig

# Configure training
config = TrainingConfig(
    batch_size=32,
    learning_rate=1e-4,
    max_epochs=100,
    curriculum_enabled=True,
    mixed_precision=True
)

# Initialize trainer
trainer = MTGTrainer(config)

# Start training
train_metrics, val_metrics = trainer.train("path/to/training_data.json")
```

### Advanced Configuration

```python
# Custom loss function weights
config = TrainingConfig(
    action_loss_weight=1.0,
    value_loss_weight=0.5,
    gradient_clip_norm=1.0,
    patience=10,
    min_delta=1e-4
)

# Curriculum learning stages
config.curriculum_stages = [
    "basic_actions",
    "strategic_decisions",
    "complex_combat",
    "advanced_tactics"
]
```

### Monitoring and Visualization

```python
from mtg_training_monitor import TrainingMonitor, MonitoringConfig

# Configure monitoring
monitor_config = MonitoringConfig(
    enable_real_time=True,
    save_plots=True,
    use_tensorboard=True,
    plot_dir="training_plots"
)

# Initialize monitor
monitor = TrainingMonitor(monitor_config)

# Start monitoring
monitor.start_training(config.__dict__)

# During training
for epoch in range(max_epochs):
    train_metrics = trainer.train_epoch()
    val_metrics = trainer.validate_epoch()
    monitor.log_epoch(epoch, train_metrics, val_metrics)
```

### Hyperparameter Optimization

```python
from mtg_hyperparameter_optimization import HyperparameterOptimizer, HyperparameterConfig

# Configure optimization
hyperopt_config = HyperparameterConfig(
    optimization_method="optuna",
    n_trials=50,
    objective_metric="val_accuracy",
    objective_direction="maximize"
)

# Run optimization
optimizer = HyperparameterOptimizer(hyperopt_config)
results = optimizer.optimize("data.json", base_config, eval_config)

print(f"Best parameters: {results['best_params']}")
print(f"Best accuracy: {results['best_value']:.3f}")
```

### Model Evaluation

```python
from mtg_evaluation_metrics import MTGEvaluator, EvaluationConfig

# Configure evaluation
eval_config = EvaluationConfig(
    compute_calibration=True,
    analyze_by_decision_type=True,
    create_confusion_matrix=True
)

# Evaluate model
evaluator = MTGEvaluator(eval_config)
results = evaluator.evaluate_model(trainer, val_loader, device)

print(f"Overall accuracy: {results['overall_metrics']['accuracy']:.3f}")
print(f"F1-score: {results['overall_metrics']['f1_weighted']:.3f}")
```

### Model Versioning and Deployment

```python
from mtg_model_versioning import ModelCheckpointManager, DeploymentValidator

# Initialize checkpoint manager
checkpoint_manager = ModelCheckpointManager("model_checkpoints")

# Save model version
model_id = checkpoint_manager.create_model_version(
    model=trainer,
    training_config=config.__dict__,
    train_metrics=train_metrics,
    val_metrics=val_metrics,
    epoch=current_epoch,
    description="Initial trained model"
)

# Validate for deployment
validator = DeploymentValidator()
validation_results = validator.validate_for_deployment(model, metadata)

if validation_results['ready_for_deployment']:
    export_path = checkpoint_manager.export_model(model_id, "torchscript")
    print(f"Model exported to: {export_path}")
```

## Training Pipeline Details

### Loss Functions

#### Outcome-Weighted Loss
The pipeline uses a sophisticated loss function that incorporates:
- **Game result importance**: Weights decisions by their impact on game outcome
- **Strategic importance**: Considers the tactical significance of decisions
- **Class imbalance handling**: Uses focal loss for rare decision types
- **Multi-task learning**: Combines action classification and value estimation

```python
class MTGOutcomeWeightedLoss(nn.Module):
    def __init__(self, action_loss_weight=1.0, value_loss_weight=0.5):
        # Focal loss parameters
        self.focal_alpha = 0.25
        self.focal_gamma = 2.0

        # Label smoothing
        self.label_smoothing = 0.1
```

#### Curriculum Learning
Training progresses through stages of increasing complexity:

1. **Basic Actions** (Epochs 0-10)
   - Land plays, mana acceleration
   - Simple combat decisions
   - Basic spell casting

2. **Strategic Decisions** (Epochs 10-25)
   - Card playing decisions
   - Removal usage
   - Counter-play strategies

3. **Complex Combat** (Epochs 25-50)
   - Multi-target combat
   - Blocking decisions
   - Combat trick usage

4. **Advanced Tactics** (Epochs 50+)
   - Combo plays
   - Resource optimization
   - Strategic sequencing

### Model Architecture

#### State Encoder
- Multi-modal transformer architecture
- Processes 282-dimension game state tensors
- Component-specific processing for board, hand/mana, phase/priority
- Position encoding for board permanents

#### Decision Head
- Shared state processing layers
- Action prediction head (16 action types)
- Value estimation head (win probability, board/card advantage)
- Strategic importance scoring
- Multi-head attention mechanism

### Training Optimization

#### Mixed Precision Training
- Uses automatic mixed precision (AMP)
- Reduces memory usage by ~50%
- Speeds up training by ~2x
- Maintains model quality

#### Gradient Clipping
- Prevents gradient explosion
- Default clip norm: 1.0
- Ensures stable training

#### Learning Rate Scheduling
- OneCycleLR scheduler
- Warmup period for stable start
- Cosine annealing for smooth convergence

### Evaluation Metrics

#### Classification Metrics
- Accuracy (overall and weighted)
- Precision, Recall, F1-score
- Confusion matrix
- Classification report

#### Calibration Metrics
- Expected Calibration Error (ECE)
- Brier score
- Reliability diagrams

#### Value Prediction Metrics
- Mean Squared Error (MSE)
- Mean Absolute Error (MAE)
- Correlation with true outcomes

#### Decision Analysis
- Performance by decision type
- Turn number analysis
- Game phase breakdown

## Performance Benchmarks

### Target Metrics
- **Overall Accuracy**: > 75%
- **F1-Score**: > 70%
- **Calibration Error**: < 0.1
- **Value Correlation**: > 0.8

### Current Performance (Sample Data)
- **Training Samples**: 1,058 decisions
- **Validation Accuracy**: 68.2%
- **Training Time**: ~15 minutes (GPU)
- **Model Size**: 52.3 MB

### Scaling Expectations
- **Full Dataset** (450K games): Expect > 80% accuracy
- **Training Time**: 4-6 hours (single GPU)
- **Model Size**: 50-100 MB

## Configuration Reference

### TrainingConfig Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `batch_size` | int | 32 | Training batch size |
| `learning_rate` | float | 1e-4 | Initial learning rate |
| `max_epochs` | int | 100 | Maximum training epochs |
| `dropout` | float | 0.1 | Dropout rate |
| `weight_decay` | float | 1e-5 | L2 regularization |
| `curriculum_enabled` | bool | True | Enable curriculum learning |
| `mixed_precision` | bool | True | Use mixed precision training |
| `gradient_clip_norm` | float | 1.0 | Gradient clipping threshold |

### MonitoringConfig Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `enable_real_time` | bool | True | Real-time monitoring |
| `save_plots` | bool | True | Save visualization plots |
| `use_tensorboard` | bool | True | Enable TensorBoard logging |
| `use_wandb` | bool | False | Enable W&B logging |
| `update_interval` | int | 10 | Real-time update interval (seconds) |

### HyperparameterConfig Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `optimization_method` | str | "optuna" | Optimization method |
| `n_trials` | int | 50 | Number of optimization trials |
| `objective_metric` | str | "val_accuracy" | Objective to optimize |
| `objective_direction` | str | "maximize" | Optimization direction |

## Troubleshooting

### Common Issues

#### CUDA Out of Memory
```python
# Reduce batch size
config.batch_size = 16

# Enable gradient checkpointing
model.gradient_checkpointing = True

# Use mixed precision
config.mixed_precision = True
```

#### Poor Convergence
```python
# Adjust learning rate
config.learning_rate = 1e-5  # Smaller

# Increase warmup
config.warmup_steps = 2000

# Add regularization
config.weight_decay = 1e-4
config.dropout = 0.2
```

#### Overfitting
```python
# Increase dropout
config.dropout = 0.3

# Add weight decay
config.weight_decay = 1e-4

# Enable early stopping
config.patience = 15
```

### Debug Mode

Enable verbose logging:
```python
import logging
logging.basicConfig(level=logging.DEBUG)

# Or use specific logger
logger = logging.getLogger('mtg_training_pipeline')
logger.setLevel(logging.DEBUG)
```

## Best Practices

### Data Preparation
1. **Balance your dataset**: Ensure representative distribution of decision types
2. **Quality over quantity**: Clean, accurate labels are more important than volume
3. **Stratified splits**: Maintain decision type distribution in train/val/test splits

### Training Strategy
1. **Start small**: Begin with simpler models and datasets
2. **Monitor closely**: Use real-time monitoring to catch issues early
3. **Save frequently**: Regular checkpoints prevent losing progress
4. **Validate rigorously**: Use comprehensive evaluation metrics

### Hyperparameter Tuning
1. **Systematic approach**: Use Bayesian optimization over grid search
2. **Early stopping**: Don't waste time on poorly performing configurations
3. **Cross-validation**: Ensure results generalize across data splits
4. **Log everything**: Track all hyperparameters and results

### Model Management
1. **Version control**: Track all model versions and their performance
2. **Metadata completeness**: Record training configurations and data versions
3. **Deployment validation**: Rigorously test models before deployment
4. **Performance monitoring**: Continue monitoring in production

## Integration with Existing Pipeline

### Task Dependencies
- **Task 2.4**: Provides training data format and preprocessing
- **Task 3.1**: Supplies transformer state encoder
- **Task 3.2**: Provides action space representation
- **Task 3.3**: Combines components into decision head

### Data Flow
```
Raw Replay Data → Task 2.4 Processing → Training Dataset →
Training Pipeline → Trained Model → Evaluation → Deployment
```

### Expected Inputs
- **Training Data**: JSON file with complete training dataset
- **Model Config**: Architecture parameters from Task 3.1
- **Action Space**: 16 action types from Task 3.2

### Expected Outputs
- **Trained Model**: PyTorch model with weights
- **Performance Metrics**: Comprehensive evaluation results
- **Training Artifacts**: Logs, plots, checkpoints
- **Deployment Package**: Exported model for production

## Future Enhancements

### Planned Features
1. **Reinforcement Learning**: Integration with self-play and RL algorithms
2. **Distributed Training**: Multi-GPU and multi-node training support
3. **Active Learning**: Intelligent sample selection for improved data efficiency
4. **Model Compression**: Quantization and pruning for deployment optimization

### Research Directions
1. **Attention Mechanisms**: Improved attention for game state understanding
2. **Memory Networks**: Long-term strategic memory implementation
3. **Meta-Learning**: Rapid adaptation to new game formats
4. **Explainable AI**: Decision interpretation and visualization

## Support and Contributing

### Getting Help
- Check existing documentation and examples
- Review troubleshooting section
- Examine log files for detailed error messages
- Use debug mode for detailed tracing

### Contributing
- Follow established code style (Black formatting)
- Add comprehensive tests for new features
- Update documentation for API changes
- Include performance benchmarks for improvements

### Bug Reports
- Include complete error messages
- Provide minimal reproducible examples
- Specify system environment and versions
- Attach relevant log files

---

*This documentation covers the complete training pipeline implementation for Task 3.4. For specific API details, refer to the inline documentation in each module.*